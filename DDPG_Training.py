{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e4948-ed9c-4fbd-a86e-20f66327a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8d6b3-a45b-4c9e-9fdd-7ae0e35335a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdNavigationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CrowdNavigationEnv, self).__init__()\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.goal = np.array([0.5, 0.5])\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state[0] += action[0] * 0.1\n",
    "        self.state[1] += action[1] * 0.1\n",
    "        distance_to_goal = np.linalg.norm(self.state[:2] - self.goal)\n",
    "        reward = -distance_to_goal\n",
    "        done = distance_to_goal < 0.1\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        # Draw goal\n",
    "        goal_circle = plt.Circle(self.goal, 0.03, color='green')\n",
    "        ax.add_patch(goal_circle)\n",
    "\n",
    "        # Draw robot\n",
    "        robot_circle = plt.Circle(self.state[:2], 0.03, color='blue')\n",
    "        ax.add_patch(robot_circle)\n",
    "\n",
    "        # Draw obstacles (assuming self.state[2:] represents obstacles)\n",
    "        obstacle_circle = plt.Circle(self.state[2:], 0.03, color='red')\n",
    "        ax.add_patch(obstacle_circle)\n",
    "\n",
    "        plt.title(\"Crowd Navigation Simulation\")\n",
    "        plt.show()\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='CrowdNavigation-v0',\n",
    "    entry_point='__main__:CrowdNavigationEnv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9575e1e-0b17-4522-803b-afd514c5232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CrowdNavigation-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80967de6-100f-4765-b5c6-8a127dd723ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.tanh(self.layer3(x)) * self.max_action\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de50bd9-d3ef-46fa-91b0-35c6c50859ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, 1)\n",
    "    \n",
    "    def forward(self, x, u):\n",
    "        x = torch.cat([x, u], 1)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10db889-6855-42a3-8c48-15e276ec6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        index = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b8663-7c7d-4578-8f19-cb08f453e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(actor, critic, target_actor, target_critic, buffer, batch_size, gamma, tau, actor_optimizer, critic_optimizer):\n",
    "    batch = buffer.sample(batch_size)\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    \n",
    "    state = torch.FloatTensor(state)\n",
    "    action = torch.FloatTensor(action)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    done = torch.FloatTensor(done).unsqueeze(1)\n",
    "    \n",
    "    target_action = target_actor(next_state)\n",
    "    target_q = target_critic(next_state, target_action)\n",
    "    target_value = reward + (1 - done) * gamma * target_q\n",
    "    critic_loss = nn.MSELoss()(critic(state, action), target_value.detach())\n",
    "    \n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    actor_loss = -critic(state, actor(state)).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c8c6-e053-48b7-9446-d71d209da392",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CrowdNavigation-v0')\n",
    "\n",
    "actor = Actor(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0], max_action=env.action_space.high[0])\n",
    "critic = Critic(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "target_actor = Actor(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0], max_action=env.action_space.high[0])\n",
    "target_critic = Critic(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "num_episodes = 2\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b5a63-435f-46b6-87fa-97f5e33d138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a73f4b-defc-484e-9416-7dab3b0e74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = actor(torch.FloatTensor(state).unsqueeze(0)).detach().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if replay_buffer.size() > batch_size:\n",
    "            ddpg_update(actor, critic, target_actor, target_critic, replay_buffer, batch_size, gamma, tau, actor_optimizer, critic_optimizer)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.plot(rewards)\n",
    "    # plt.xlabel('Episode')\n",
    "    # plt.ylabel('Reward')\n",
    "    # plt.title('Training Reward Over Episodes')\n",
    "    # plt.savefig(f'plots/rewards_{episode}.png')\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fd098-c274-45f1-99e5-4901e69799ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c8314-7075-4bdf-8692-159b6496593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89499f7-7c2c-4028-8832-075518990db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e2eba-fa4a-4c41-94d2-75482c683734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d686c-87b3-4459-b36b-7191c58de90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21b077-59e8-4754-b8b6-60e24b7915a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a90bf3-bf34-4467-a1ab-99c1bbd248fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0867d-5f1b-498b-afb2-d50821244a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
