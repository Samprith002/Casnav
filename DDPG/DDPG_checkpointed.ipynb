{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fdfec-890e-4c78-b935-2e67a4a3318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym  # Assuming you have a gym-like environment\n",
    "from gym import spaces\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import imageio.v2 as imageio\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import FancyArrow\n",
    "from shapely.geometry import Point, LineString, box\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b0bb-c940-4c64-a007-f132a0afd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousRobotNavigationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def is_goal_behind_wall(self, goal_position):\n",
    "        # Check if goal_position is behind any wall\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if goal_position[0] >= x and goal_position[0] <= x + width \\\n",
    "                    and goal_position[1] >= y and goal_position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "    def __init__(self):\n",
    "        super(ContinuousRobotNavigationEnv, self).__init__()\n",
    "        self.grid_size = 10\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size, shape=(2,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)  # Continuous action space\n",
    "\n",
    "        self.velocity_scale = 0.3\n",
    "        self.lidar_range = 3.0\n",
    "\n",
    "        self.num_humans = 5  # Number of humans\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        \n",
    "        # self.num_obstacles = 3  # Number of static obstacles\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "\n",
    "        self.maze_walls = [\n",
    "            (5, 8, 0.3, 1.5),   # Example wall 1\n",
    "            (0, 7, 1.5, 0.3),   # Example wall 2\n",
    "            (6.5, 6.5, 1.5, 0.3),   # Example wall 3\n",
    "            (4, 4.5, 0.3, 1.5),   # Example wall 4\n",
    "            (1.5, 2, 0.3, 1.5), #5th wall\n",
    "            (5, 2.5, 1.5, 0.3),  #6th wall\n",
    "            (8, 4, 1.5, 0.3),    #7th wall\n",
    "            (7, 0, 0.3, 1.5)   #8th wall    \n",
    "        ]\n",
    "        # self.maze_walls = [\n",
    "        #     (0, 9.5, 5, 0.5),    # Top horizontal wall\n",
    "        #     (5.5, 9.5, 4.5, 0.5), # Top-right horizontal wall\n",
    "        #     (9.5, 6, 0.5, 3.5),   # Right vertical wall\n",
    "        #     (0, 0, 0.5, 5),       # Left vertical wall\n",
    "        #     (0, 5.5, 0.5, 4.5),   # Left vertical wall 2\n",
    "        #     (3, 0, 7, 0.5),       # Bottom horizontal wall\n",
    "        #     (2, 4, 0.5, 5),       # Vertical wall in center-left\n",
    "        #     (3, 2, 4, 0.5),       # Horizontal wall in center-bottom\n",
    "        #     (6, 3, 0.5, 3),       # Vertical wall in center-right\n",
    "        #     (6, 6, 4, 0.5)        # Horizontal wall in center-right\n",
    "        # ]\n",
    "\n",
    "        \n",
    "        \n",
    "        self.goal_position = (0, 0)\n",
    "\n",
    "        self.frames = []  # Initialize the frames list\n",
    "\n",
    "        \n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _random_position(self):\n",
    "        return [np.random.uniform(0, self.grid_size - 1), np.random.uniform(0, self.grid_size - 1)]\n",
    "\n",
    "    def _random_velocity(self):\n",
    "        # return [np.random.uniform(-1, 1), np.random.uniform(-1, 1)]\n",
    "        return [np.random.uniform(-1, 1) * self.velocity_scale, np.random.uniform(-1, 1) * self.velocity_scale]\n",
    "\n",
    "    def reset(self):\n",
    "        self.robot_position = [0, 0]\n",
    "        self.robot_orientation = np.pi/4  #looking diagonally up\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # self.goal_position = self._random_position()\n",
    "        while True:\n",
    "            self.goal_position = self._random_position()\n",
    "            if not self.is_goal_behind_wall(self.goal_position):\n",
    "                break\n",
    "        self.frames = []  # Reset frames list at the start of each episode\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def check_wall_collision(self, position):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.1) and position[0] <= x + width and position[1] >= (y-0.1) and position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def predict_human_positions(self):\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]  # Reset predicted positions\n",
    "        num_predictions = 5  # Number of future positions to predict\n",
    "        \n",
    "        for i in range(self.num_humans):\n",
    "            future_positions = []\n",
    "            future_position = np.array(self.human_positions[i])\n",
    "            for _ in range(num_predictions):\n",
    "                future_position += np.array(self.human_velocities[i])\n",
    "                future_position[0] = np.clip(future_position[0], 0, self.grid_size - 1)\n",
    "                future_position[1] = np.clip(future_position[1], 0, self.grid_size - 1)\n",
    "                future_positions.append(future_position.copy())\n",
    "        \n",
    "            # Update current position\n",
    "            self.human_positions[i][0] += self.human_velocities[i][0]\n",
    "            self.human_positions[i][1] += self.human_velocities[i][1]\n",
    "        \n",
    "            # Check for boundary collisions and reverse velocity if necessary\n",
    "            if self.human_positions[i][0] < 0 or self.human_positions[i][0] >= self.grid_size:\n",
    "                self.human_velocities[i][0] *= -1\n",
    "            if self.human_positions[i][1] < 0 or self.human_positions[i][1] >= self.grid_size:\n",
    "                self.human_velocities[i][1] *= -1\n",
    "\n",
    "            if self.check_wall_collision(self.human_positions[i]):\n",
    "                self.human_velocities[i][0] *= -1\n",
    "                self.human_velocities[i][1] *= -1\n",
    "        \n",
    "            # Update future positions if within lidar range\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(self.human_positions[i])) <= self.lidar_range:\n",
    "                self.predicted_human_positions[i] = future_positions\n",
    "        \n",
    "\n",
    "\n",
    "    def check_collision(self, position):\n",
    "        # Check collision with humans\n",
    "        for human_pos in self.human_positions:\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(human_pos)) < 1.0:  # Collision threshold\n",
    "                return True\n",
    "        \n",
    "        # Check collision with obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     if np.linalg.norm(np.array(self.robot_position) - np.array(obstacle_pos)) < 1.0:  # Collision threshold\n",
    "        #         return True\n",
    "\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.3) and position[0] <= x + width and position[1] >= (y-0.3) and position[1] <= y + height:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # Apply scaling to the action to reduce velocity\n",
    "        scaled_action = np.array(action) * self.velocity_scale\n",
    "    \n",
    "        # Calculate the new robot position\n",
    "        new_robot_position = [\n",
    "            self.robot_position[0] + scaled_action[0],\n",
    "            self.robot_position[1] + scaled_action[1]\n",
    "        ]\n",
    "    \n",
    "        # Clip the new position to stay within the environment bounds\n",
    "        new_robot_position = [\n",
    "            np.clip(new_robot_position[0], 0, self.grid_size - 1),\n",
    "            np.clip(new_robot_position[1], 0, self.grid_size - 1)\n",
    "        ]\n",
    "    \n",
    "        # Check if the new position collides with maze walls\n",
    "        if not self.check_collision(new_robot_position):\n",
    "            self.robot_position = new_robot_position\n",
    "        else:\n",
    "            done = True\n",
    "            return self.state, -10, done, {}\n",
    "    \n",
    "        if np.linalg.norm(scaled_action) > 0:\n",
    "            self.robot_orientation = np.arctan2(scaled_action[1], scaled_action[0])\n",
    "    \n",
    "        self.predict_human_positions()\n",
    "        \n",
    "        goal_threshold = 0.7\n",
    "        distance_to_goal = np.linalg.norm(np.array(self.robot_position) - np.array(self.goal_position))\n",
    "\n",
    "        reward = -distance_to_goal\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if distance_to_goal < 0.7: \n",
    "            reward += 100  \n",
    "            done = True\n",
    "        else:\n",
    "            collision = self.check_collision(self.robot_position)\n",
    "            if collision:\n",
    "                reward -= 10  \n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "    \n",
    "        # if done:\n",
    "        #     self.reset()  \n",
    "        \n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # print(self.state)\n",
    "        info = {}  # Add any additional info you want to pass\n",
    "    \n",
    "        return self.state, reward, done, info\n",
    "    def render(self, mode='human'):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "        # Draw the goal position\n",
    "        goal = plt.Circle(self.goal_position, 0.5, color='green')\n",
    "        ax.add_artist(goal)\n",
    "    \n",
    "        # Draw the robot\n",
    "        robot = plt.Circle(self.robot_position, 0.5, color='red')\n",
    "        ax.add_artist(robot)\n",
    "    \n",
    "        # Draw the humans\n",
    "        for human_pos in self.human_positions:\n",
    "            human = plt.Circle(human_pos, 0.5, color='blue')\n",
    "            ax.add_artist(human)\n",
    "\n",
    "        \n",
    "        #static obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     obstacle = plt.Circle(obstacle_pos, 0.5, color='black')\n",
    "        #     ax.add_artist(obstacle)\n",
    "\n",
    "        #walls\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            rect = plt.Rectangle((x, y), width, height, color='black')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "        \n",
    "        #Orientation\n",
    "        arrow_length = 0.7\n",
    "        dx = arrow_length * np.cos(self.robot_orientation)\n",
    "        dy = arrow_length * np.sin(self.robot_orientation)\n",
    "        arrow = FancyArrow(self.robot_position[0], self.robot_position[1], dx, dy, color='black', width=0.1)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "        #Lidar range\n",
    "        # lidar_circle = plt.Circle(self.robot_position, self.lidar_range, color='gray', alpha=0.2)\n",
    "        # ax.add_artist(lidar_circle)\n",
    "\n",
    "        self.render_lidar(ax)\n",
    "        # Plot observation range, ensuring it doesn't pass through walls\n",
    "    \n",
    "        # def intersects_wall(start, end):\n",
    "        #     line = LineString([start, end])\n",
    "        #     for wall in self.maze_walls:\n",
    "        #         wall_box = box(wall[0], wall[1], wall[0] + wall[2], wall[1] + wall[3])\n",
    "        #         if line.intersects(wall_box):\n",
    "        #             return True\n",
    "        #     return False\n",
    "    \n",
    "        # lidar_R = self.lidar_range\n",
    "        # angles = np.linspace(0, 2 * np.pi, 360)\n",
    "        # x0, y0 = self.robot_position\n",
    "    \n",
    "        # for angle in angles:\n",
    "        #     x1 = x0 + lidar_R * np.cos(angle)\n",
    "        #     y1 = y0 + lidar_R * np.sin(angle)\n",
    "        #     if not intersects_wall((x0, y0), (x1, y1)):\n",
    "        #         line = plt.Line2D((x0, x1), (y0, y1), color='gray', alpha=0.2)\n",
    "        #         ax.add_line(line)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #Trajectory prediction\n",
    "        for predicted_positions in self.predicted_human_positions:\n",
    "            for future_pos in predicted_positions:\n",
    "                future_circle = plt.Circle(future_pos, 0.2, color='pink', alpha=0.3)\n",
    "                ax.add_artist(future_circle)\n",
    "\n",
    "        \n",
    "    \n",
    "        plt.axis('off')\n",
    "    \n",
    "        # Save frame to list\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        frame = imageio.imread(buf)\n",
    "        self.frames.append(frame)\n",
    "        buf.close()\n",
    "\n",
    "        # Show the plot in real-time\n",
    "        # plt.imshow(frame)\n",
    "        # plt.axis('off')\n",
    "        # plt.pause(0.1)\n",
    "        # plt.clf()\n",
    "\n",
    "    def render_lidar(self, ax):\n",
    "        lidar_segments = 36\n",
    "        angle_increment = 2 * np.pi / lidar_segments\n",
    "        for i in range(lidar_segments):\n",
    "            angle = i * angle_increment\n",
    "            end_point = self.robot_position + np.array([np.cos(angle), np.sin(angle)]) * self.lidar_range\n",
    "            ray_end_point = self.get_ray_end_point(self.robot_position, end_point)\n",
    "            ax.plot([self.robot_position[0], ray_end_point[0]], [self.robot_position[1], ray_end_point[1]], color='dimgray', alpha=0.2)\n",
    "\n",
    "    def get_ray_end_point(self, start, end):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if self.line_intersects_rect(start, end, x, y, width, height):\n",
    "                return self.get_intersection_point(start, end, x, y, width, height)\n",
    "        return end\n",
    "\n",
    "    def line_intersects_rect(self, start, end, x, y, width, height):\n",
    "        def ccw(A, B, C):\n",
    "            return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "\n",
    "        A = start\n",
    "        B = end\n",
    "        C1 = [x, y]\n",
    "        C2 = [x + width, y]\n",
    "        C3 = [x, y + height]\n",
    "        C4 = [x + width, y + height]\n",
    "        return (ccw(A, C1, C2) != ccw(B, C1, C2) and ccw(A, B, C1) != ccw(A, B, C2)) or \\\n",
    "               (ccw(A, C3, C4) != ccw(B, C3, C4) and ccw(A, B, C3) != ccw(A, B, C4)) or \\\n",
    "               (ccw(A, C1, C3) != ccw(B, C1, C3) and ccw(A, B, C1) != ccw(A, B, C3)) or \\\n",
    "               (ccw(A, C2, C4) != ccw(B, C2, C4) and ccw(A, B, C2) != ccw(A, B, C4))\n",
    "\n",
    "    def get_intersection_point(self, start, end, x, y, width, height):\n",
    "        def line(p1, p2):\n",
    "            A = (p1[1] - p2[1])\n",
    "            B = (p2[0] - p1[0])\n",
    "            C = (p1[0] * p2[1] - p2[0] * p1[1])\n",
    "            return A, B, -C\n",
    "\n",
    "        def intersection(L1, L2):\n",
    "            D = L1[0] * L2[1] - L1[1] * L2[0]\n",
    "            Dx = L1[2] * L2[1] - L1[1] * L2[2]\n",
    "            Dy = L1[0] * L2[2] - L1[2] * L2[0]\n",
    "            if D != 0:\n",
    "                return Dx / D, Dy / D\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        walls = [\n",
    "            ([x, y], [x + width, y]),\n",
    "            ([x, y], [x, y + height]),\n",
    "            ([x + width, y], [x + width, y + height]),\n",
    "            ([x, y + height], [x + width, y + height])\n",
    "        ]\n",
    "        closest_point = end\n",
    "        min_distance = np.linalg.norm(np.array(start) - np.array(end))\n",
    "        for wall in walls:\n",
    "            L1 = line(start, end)\n",
    "            L2 = line(wall[0], wall[1])\n",
    "            R = intersection(L1, L2)\n",
    "            if R:\n",
    "                dist = np.linalg.norm(np.array(start) - np.array(R))\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_point = R\n",
    "        return closest_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e16ae5-a77a-4f6a-869f-e56b224ebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)  # Project back to the original dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query_layer(x)\n",
    "        keys = self.key_layer(x)\n",
    "        values = self.value_layer(x)\n",
    "        \n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(queries.size(-1))\n",
    "        attention_weights = self.softmax(scores)\n",
    "        \n",
    "        attended_values = torch.matmul(attention_weights, values)\n",
    "        attended_values = self.output_layer(attended_values)  # Project back to the original dimension\n",
    "        return attended_values, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052806a-327a-4265-9989-df1bd4c1efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Output action is in range [-1, 1]\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649e0fe-ad28-4d7d-8092-1b9188ce2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, tau=1e-3):\n",
    "        self.actor = Actor(state_size, action_size)\n",
    "        self.actor_target = Actor(state_size, action_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(state_size, action_size)\n",
    "        self.critic_target = Critic(state_size, action_size)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, batch_size, state_size, action_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize target networks with same weights as original networks\n",
    "        self._update_targets(self.actor_target, self.actor)\n",
    "        self._update_targets(self.critic_target, self.critic)\n",
    "    \n",
    "    def _update_targets(self, target_model, model):\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        return self.actor(state).detach().numpy()\n",
    "        \n",
    "    #Test\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    #Learn\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        \n",
    "        # Critic update\n",
    "        Q_targets_next = self.critic_target(next_states, self.actor_target(next_states))\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets.detach())\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor update\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        self._soft_update_target(self.actor_target, self.actor, self.tau)\n",
    "        self._soft_update_target(self.critic_target, self.critic, self.tau)\n",
    "    \n",
    "    def _soft_update_target(self, target_model, model, tau):\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e172b2-ae52-4933-903b-83a0495d42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, batch_size, state_size, action_size):\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer = []\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.idx] = experience\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in batch])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ad112-fa1d-4679-a06c-1ea38a18a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(actor, critic, actor_target, critic_target, actor_optimizer, critic_optimizer, filepath):\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_target_state_dict': actor_target.state_dict(),\n",
    "        'critic_target_state_dict': critic_target.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, actor, critic, actor_target, critic_target, actor_optimizer, critic_optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    actor_target.load_state_dict(checkpoint['actor_target_state_dict'])\n",
    "    critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "    actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "    critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "    print(f\"Checkpoint loaded from {filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12af99-38c2-41db-ab20-742c3b7595dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = ContinuousRobotNavigationEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    buffer_size = 10000\n",
    "    batch_size = 64\n",
    "    lr_actor = 1e-3\n",
    "    lr_critic = 1e-3\n",
    "    gamma = 0.99\n",
    "    tau = 1e-3\n",
    "\n",
    "    ddpg_agent = DDPGAgent(state_dim, action_dim, buffer_size, batch_size, lr_actor, lr_critic, gamma, tau)\n",
    "    replay_buffer = ReplayBuffer(max_size=buffer_size, batch_size=batch_size, state_size=state_dim, action_size=action_dim)\n",
    "\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Define checkpoint filename with timestamp\n",
    "    checkpoint_filename = f\"ddpg_checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\"\n",
    "    checkpoint_filepath = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "\n",
    "    # if os.path.exists(checkpoint_filepath):\n",
    "    #     load_checkpoint(checkpoint_filepath, ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target,\n",
    "    #                     ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer)\n",
    "    \n",
    "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')], reverse=True)\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[0]) if checkpoint_files else None\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Loading checkpoint {latest_checkpoint}\")\n",
    "        load_checkpoint(latest_checkpoint, ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target,\n",
    "                        ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer)\n",
    "\n",
    "    # Training loop\n",
    "    num_episodes = 150\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        env.frames = []  # Clear frames for the new episode\n",
    "        for t in range(100):\n",
    "            action = ddpg_agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ddpg_agent.remember(state, action, reward, next_state, done)  # Remember the experience\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            env.render()  \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % 50 == 0:  # Save checkpoint every 100 episodes\n",
    "            save_checkpoint(ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target, \n",
    "                            ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer, checkpoint_filepath)\n",
    "            print(f'Saved checkpoint at episode {episode}')\n",
    "        \n",
    "        # Learn from the experiences in the replay buffer\n",
    "        ddpg_agent.learn()\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "        # Save GIF\n",
    "        imageio.mimsave(f'episode_{episode}.gif', env.frames, fps=10)\n",
    "\n",
    "    plt.plot(range(num_episodes), episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66c1fd-6650-4491-ba4d-1725e142c19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bea233-7938-49e6-8ea8-94917ed88da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
