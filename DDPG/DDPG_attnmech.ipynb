{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fdfec-890e-4c78-b935-2e67a4a3318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym  # Assuming you have a gym-like environment\n",
    "from gym import spaces\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import imageio.v2 as imageio\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import FancyArrow\n",
    "from shapely.geometry import Point, LineString, box\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b0bb-c940-4c64-a007-f132a0afd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousRobotNavigationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def is_goal_behind_wall(self, goal_position):\n",
    "        # Check if goal_position is behind any wall\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if goal_position[0] >= x and goal_position[0] <= x + width \\\n",
    "                    and goal_position[1] >= y and goal_position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "    def __init__(self):\n",
    "        super(ContinuousRobotNavigationEnv, self).__init__()\n",
    "        self.grid_size = 10\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size, shape=(2,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)  # Continuous action space\n",
    "\n",
    "        self.velocity_scale = 0.3\n",
    "        self.lidar_range = 3.0\n",
    "\n",
    "        self.num_humans = 5  # Number of humans\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        \n",
    "        # self.num_obstacles = 3  # Number of static obstacles\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "\n",
    "        # self.maze_walls = [\n",
    "        #     (5, 8, 0.3, 1.5),   # Example wall 1\n",
    "        #     (0, 7, 1.5, 0.3),   # Example wall 2\n",
    "        #     (6.5, 6.5, 1.5, 0.3),   # Example wall 3\n",
    "        #     (4, 4.5, 0.3, 1.5),   # Example wall 4\n",
    "        #     (1.5, 2, 0.3, 1.5), #5th wall\n",
    "        #     (5, 2.5, 1.5, 0.3),  #6th wall\n",
    "        #     (8, 4, 1.5, 0.3),    #7th wall\n",
    "        #     (7, 0, 0.3, 1.5)   #8th wall    \n",
    "        # ]\n",
    "\n",
    "        self.maze_walls = [\n",
    "            #(5, 8, 0.3, 1),   # Example wall 1\n",
    "            # (0, 7, 1, 0.3),   # Example wall 2\n",
    "            # (6.5, 6.5, 1, 0.3),   # Example wall 3\n",
    "            # # (4, 4.5, 0.3, 1),   # Example wall 4\n",
    "            # (1.5, 2, 0.3, 1), #5th wall\n",
    "            # (5, 2.5, 1, 0.3),  #6th wall\n",
    "            # (8, 4, 1, 0.3),    #7th wall\n",
    "            # (7, 0, 0.3, 1),    #8th wall\n",
    "            # (0, 0, self.grid_size, 0.3),               # Bottom boundary wall\n",
    "            # (0, self.grid_size - 0.3, self.grid_size, 0.3),  # Top boundary wall\n",
    "            # (0, 0, 0.3, self.grid_size),               # Left boundary wall\n",
    "            # (self.grid_size - 0.3, 0, 0.3, self.grid_size)  # Right boundary wall#8th wall    \n",
    "        ]\n",
    "        \n",
    "        self.goal_position = (0, 0)\n",
    "\n",
    "        self.frames = []  # Initialize the frames list\n",
    "\n",
    "        \n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _random_position(self):\n",
    "        return [np.random.uniform(0, self.grid_size - 1), np.random.uniform(0, self.grid_size - 1)]\n",
    "\n",
    "    def _random_velocity(self):\n",
    "        # return [np.random.uniform(-1, 1), np.random.uniform(-1, 1)]\n",
    "        return [np.random.uniform(-1, 1) * self.velocity_scale, np.random.uniform(-1, 1) * self.velocity_scale]\n",
    "\n",
    "    def reset(self):\n",
    "        self.robot_position = [0, 0]\n",
    "        self.robot_orientation = np.pi/4  #looking diagonally up\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # self.goal_position = self._random_position()\n",
    "        # while True:\n",
    "        #     self.goal_position = self._random_position()\n",
    "        #     if not self.is_goal_behind_wall(self.goal_position):\n",
    "        #         break\n",
    "        self.goal_position = (6,4)\n",
    "        self.frames = []  # Reset frames list at the start of each episode\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def check_wall_collision(self, position):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.1) and position[0] <= x + width and position[1] >= (y-0.1) and position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def predict_human_positions(self):\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]  # Reset predicted positions\n",
    "        num_predictions = 5  # Number of future positions to predict\n",
    "        \n",
    "        for i in range(self.num_humans):\n",
    "            future_positions = []\n",
    "            future_position = np.array(self.human_positions[i])\n",
    "            for _ in range(num_predictions):\n",
    "                future_position += np.array(self.human_velocities[i])\n",
    "                future_position[0] = np.clip(future_position[0], 0, self.grid_size - 1)\n",
    "                future_position[1] = np.clip(future_position[1], 0, self.grid_size - 1)\n",
    "                future_positions.append(future_position.copy())\n",
    "        \n",
    "            # Update current position\n",
    "            self.human_positions[i][0] += self.human_velocities[i][0]\n",
    "            self.human_positions[i][1] += self.human_velocities[i][1]\n",
    "        \n",
    "            # Check for boundary collisions and reverse velocity if necessary\n",
    "            if self.human_positions[i][0] < 0 or self.human_positions[i][0] >= self.grid_size:\n",
    "                self.human_velocities[i][0] *= -1\n",
    "            if self.human_positions[i][1] < 0 or self.human_positions[i][1] >= self.grid_size:\n",
    "                self.human_velocities[i][1] *= -1\n",
    "\n",
    "            if self.check_wall_collision(self.human_positions[i]):\n",
    "                self.human_velocities[i][0] *= -1\n",
    "                self.human_velocities[i][1] *= -1\n",
    "        \n",
    "            # Update future positions if within lidar range\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(self.human_positions[i])) <= self.lidar_range:\n",
    "                self.predicted_human_positions[i] = future_positions\n",
    "        \n",
    "\n",
    "\n",
    "    def check_collision(self, position):\n",
    "        # Check collision with humans\n",
    "        for human_pos in self.human_positions:\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(human_pos)) < 1.0:  # Collision threshold\n",
    "                return True\n",
    "        \n",
    "        # Check collision with obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     if np.linalg.norm(np.array(self.robot_position) - np.array(obstacle_pos)) < 1.0:  # Collision threshold\n",
    "        #         return True\n",
    "\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.3) and position[0] <= x + width and position[1] >= (y-0.3) and position[1] <= y + height:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # Apply scaling to the action to reduce velocity\n",
    "        scaled_action = np.array(action) * self.velocity_scale\n",
    "    \n",
    "        # Calculate the new robot position\n",
    "        new_robot_position = [\n",
    "            self.robot_position[0] + scaled_action[0],\n",
    "            self.robot_position[1] + scaled_action[1]\n",
    "        ]\n",
    "    \n",
    "        # Clip the new position to stay within the environment bounds\n",
    "        new_robot_position = [\n",
    "            np.clip(new_robot_position[0], 0, self.grid_size - 1),\n",
    "            np.clip(new_robot_position[1], 0, self.grid_size - 1)\n",
    "        ]\n",
    "    \n",
    "        # Check if the new position collides with maze walls\n",
    "        if not self.check_collision(new_robot_position):\n",
    "            self.robot_position = new_robot_position\n",
    "        else:\n",
    "            done = True\n",
    "            return self.state, -10, done, {}\n",
    "    \n",
    "        if np.linalg.norm(scaled_action) > 0:\n",
    "            self.robot_orientation = np.arctan2(scaled_action[1], scaled_action[0])\n",
    "    \n",
    "        self.predict_human_positions()\n",
    "        \n",
    "        goal_threshold = 0.7\n",
    "        distance_to_goal = np.linalg.norm(np.array(self.robot_position) - np.array(self.goal_position))\n",
    "\n",
    "        reward = -distance_to_goal\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if distance_to_goal < 0.7: \n",
    "            reward += 100  \n",
    "            done = True\n",
    "        else:\n",
    "            collision = self.check_collision(self.robot_position)\n",
    "            if collision:\n",
    "                reward -= 10  \n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "    \n",
    "        # if done:\n",
    "        #     self.reset()  \n",
    "        \n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # print(self.state)\n",
    "        info = {}  # Add any additional info you want to pass\n",
    "    \n",
    "        return self.state, reward, done, info\n",
    "    def render(self, mode='human'):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "        # Draw the goal position\n",
    "        goal = plt.Circle(self.goal_position, 0.5, color='green')\n",
    "        ax.add_artist(goal)\n",
    "    \n",
    "        # Draw the robot\n",
    "        robot = plt.Circle(self.robot_position, 0.5, color='red')\n",
    "        ax.add_artist(robot)\n",
    "    \n",
    "        # Draw the humans\n",
    "        for human_pos in self.human_positions:\n",
    "            human = plt.Circle(human_pos, 0.5, color='blue')\n",
    "            ax.add_artist(human)\n",
    "\n",
    "        \n",
    "        #static obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     obstacle = plt.Circle(obstacle_pos, 0.5, color='black')\n",
    "        #     ax.add_artist(obstacle)\n",
    "\n",
    "        #walls\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            rect = plt.Rectangle((x, y), width, height, color='black')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "        \n",
    "        #Orientation\n",
    "        arrow_length = 0.7\n",
    "        dx = arrow_length * np.cos(self.robot_orientation)\n",
    "        dy = arrow_length * np.sin(self.robot_orientation)\n",
    "        arrow = FancyArrow(self.robot_position[0], self.robot_position[1], dx, dy, color='black', width=0.1)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "        self.render_lidar(ax)\n",
    "\n",
    "        #Trajectory prediction\n",
    "        for predicted_positions in self.predicted_human_positions:\n",
    "            for future_pos in predicted_positions:\n",
    "                future_circle = plt.Circle(future_pos, 0.2, color='pink', alpha=0.3)\n",
    "                ax.add_artist(future_circle)\n",
    "\n",
    "        \n",
    "    \n",
    "        plt.axis('off')\n",
    "    \n",
    "        # Save frame to list\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        frame = imageio.imread(buf)\n",
    "        self.frames.append(frame)\n",
    "        buf.close()\n",
    "        plt.close() \n",
    "\n",
    "        # Show the plot in real-time\n",
    "        # plt.imshow(frame)\n",
    "        # plt.axis('off')\n",
    "        # plt.pause(0.1)\n",
    "        # plt.clf()\n",
    "\n",
    "    def render_lidar(self, ax):\n",
    "        lidar_segments = 36\n",
    "        angle_increment = 2 * np.pi / lidar_segments\n",
    "        for i in range(lidar_segments):\n",
    "            angle = i * angle_increment\n",
    "            end_point = self.robot_position + np.array([np.cos(angle), np.sin(angle)]) * self.lidar_range\n",
    "            ray_end_point = self.get_ray_end_point(self.robot_position, end_point)\n",
    "            ax.plot([self.robot_position[0], ray_end_point[0]], [self.robot_position[1], ray_end_point[1]], color='dimgray', alpha=0.2)\n",
    "\n",
    "    def get_ray_end_point(self, start, end):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if self.line_intersects_rect(start, end, x, y, width, height):\n",
    "                return self.get_intersection_point(start, end, x, y, width, height)\n",
    "        return end\n",
    "\n",
    "    def line_intersects_rect(self, start, end, x, y, width, height):\n",
    "        def ccw(A, B, C):\n",
    "            return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "\n",
    "        A = start\n",
    "        B = end\n",
    "        C1 = [x, y]\n",
    "        C2 = [x + width, y]\n",
    "        C3 = [x, y + height]\n",
    "        C4 = [x + width, y + height]\n",
    "        return (ccw(A, C1, C2) != ccw(B, C1, C2) and ccw(A, B, C1) != ccw(A, B, C2)) or \\\n",
    "               (ccw(A, C3, C4) != ccw(B, C3, C4) and ccw(A, B, C3) != ccw(A, B, C4)) or \\\n",
    "               (ccw(A, C1, C3) != ccw(B, C1, C3) and ccw(A, B, C1) != ccw(A, B, C3)) or \\\n",
    "               (ccw(A, C2, C4) != ccw(B, C2, C4) and ccw(A, B, C2) != ccw(A, B, C4))\n",
    "\n",
    "    def get_intersection_point(self, start, end, x, y, width, height):\n",
    "        def line(p1, p2):\n",
    "            A = (p1[1] - p2[1])\n",
    "            B = (p2[0] - p1[0])\n",
    "            C = (p1[0] * p2[1] - p2[0] * p1[1])\n",
    "            return A, B, -C\n",
    "\n",
    "        def intersection(L1, L2):\n",
    "            D = L1[0] * L2[1] - L1[1] * L2[0]\n",
    "            Dx = L1[2] * L2[1] - L1[1] * L2[2]\n",
    "            Dy = L1[0] * L2[2] - L1[2] * L2[0]\n",
    "            if D != 0:\n",
    "                return Dx / D, Dy / D\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        walls = [\n",
    "            ([x, y], [x + width, y]),\n",
    "            ([x, y], [x, y + height]),\n",
    "            ([x + width, y], [x + width, y + height]),\n",
    "            ([x, y + height], [x + width, y + height])\n",
    "        ]\n",
    "        closest_point = end\n",
    "        min_distance = np.linalg.norm(np.array(start) - np.array(end))\n",
    "        for wall in walls:\n",
    "            L1 = line(start, end)\n",
    "            L2 = line(wall[0], wall[1])\n",
    "            R = intersection(L1, L2)\n",
    "            if R:\n",
    "                dist = np.linalg.norm(np.array(start) - np.array(R))\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_point = R\n",
    "        return closest_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e16ae5-a77a-4f6a-869f-e56b224ebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Compute attention scores and weights\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (x.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute attention output\n",
    "        attention_output = torch.bmm(attention_weights, V)\n",
    "        attention_output = self.fc_out(attention_output)\n",
    "        \n",
    "        return attention_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052806a-327a-4265-9989-df1bd4c1efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128, num_humans=5):\n",
    "        super(Actor, self).__init__()\n",
    "        self.num_humans = num_humans\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.attention = AttentionModule(2, hidden_size)  # Each human has 2 dimensions (x, y)\n",
    "        self.goal_attention = AttentionModule(2, hidden_size)  # Goal also has 2 dimensions (x, y)\n",
    "        \n",
    "        # The input size to fc1 is:\n",
    "        # state_size + hidden_size (from humans) + hidden_size (from goal)\n",
    "        self.fc1 = nn.Linear(state_size + hidden_size + hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, state, humans, goal):\n",
    "        # Reshape humans for attention module\n",
    "        humans = humans.view(-1, self.num_humans, 2)  # Reshape to (batch_size, num_humans, 2)\n",
    "        \n",
    "        # Apply attention to humans\n",
    "        human_attn_output, human_attn_weights = self.attention(humans)\n",
    "        \n",
    "        # Apply attention to goal\n",
    "        goal = goal.unsqueeze(1)  # Add extra dimension for compatibility\n",
    "        goal_attn_output, goal_attn_weights = self.goal_attention(goal)\n",
    "        \n",
    "        # Concatenate state with attention outputs\n",
    "        x = torch.cat((state, human_attn_output.mean(1), goal_attn_output.squeeze(1)), dim=-1)\n",
    "        \n",
    "        # print(f\"Shape of x before fc1: {x.shape}\")  # Debugging line\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Output action is in range [-1, 1]\n",
    "        return x, human_attn_weights, goal_attn_weights\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, human_size, goal_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.human_attention = nn.Linear(human_size, human_size)  # Replace with actual attention mechanism\n",
    "        self.goal_attention = nn.Linear(goal_size, goal_size)    # Replace with actual attention mechanism\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size + human_size + goal_size + action_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action, humans, goal):\n",
    "        # Compute attention outputs\n",
    "        human_attn_output = self.human_attention(humans)\n",
    "        goal_attn_output = self.goal_attention(goal)\n",
    "        \n",
    "        # Ensure outputs are tensors and handle the dimensions correctly\n",
    "        if isinstance(human_attn_output, tuple):\n",
    "            human_attn_output = human_attn_output[0]\n",
    "        if isinstance(goal_attn_output, tuple):\n",
    "            goal_attn_output = goal_attn_output[0]\n",
    "        \n",
    "        # Ensure the dimensions are compatible for concatenation\n",
    "        state = state.flatten(start_dim=1)\n",
    "        action = action.flatten(start_dim=1) if isinstance(action, torch.Tensor) else torch.tensor(action).flatten(start_dim=1)\n",
    "        \n",
    "        # Adjust attention outputs\n",
    "        if len(human_attn_output.shape) == 3:\n",
    "            human_attn_output = human_attn_output.mean(dim=1)\n",
    "        if len(goal_attn_output.shape) == 3:\n",
    "            goal_attn_output = goal_attn_output.squeeze(1)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        x = torch.cat((state, human_attn_output, goal_attn_output, action), dim=-1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ae599-a80e-4b99-a426-09afaa19b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=0.15, dt=1e-2, initial_state=None):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        self.initial_state = initial_state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.initial_state) if self.initial_state is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.randn(*self.mu.shape)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e172b2-ae52-4933-903b-83a0495d42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, batch_size, state_size, action_size, human_size, goal_size):\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.human_size = human_size\n",
    "        self.goal_size = goal_size\n",
    "        self.buffer = [None] * max_size\n",
    "        self.idx = 0\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, humans, next_humans, goal):\n",
    "        experience = (state, action, reward, next_state, done, humans, next_humans, goal)\n",
    "        self.buffer[self.idx] = experience\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        self.current_size = min(self.current_size + 1, self.max_size)\n",
    "\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(self.current_size, batch_size, replace=False)\n",
    "        states = np.zeros((batch_size, self.state_size))\n",
    "        actions = np.zeros((batch_size, self.action_size))\n",
    "        rewards = np.zeros(batch_size)\n",
    "        next_states = np.zeros((batch_size, self.state_size))\n",
    "        dones = np.zeros(batch_size)\n",
    "        humans = np.zeros((batch_size, self.human_size))\n",
    "        next_humans = np.zeros((batch_size, self.human_size))\n",
    "        goals = np.zeros((batch_size, self.goal_size))\n",
    "    \n",
    "        for i, idx in enumerate(batch):\n",
    "            states[i] = self.buffer[idx][0]\n",
    "            actions[i] = self.buffer[idx][1]\n",
    "            rewards[i] = self.buffer[idx][2]\n",
    "            next_states[i] = self.buffer[idx][3]\n",
    "            dones[i] = self.buffer[idx][4]\n",
    "            humans[i] = self.buffer[idx][5]\n",
    "            next_humans[i] = self.buffer[idx][6]\n",
    "            goals[i] = self.buffer[idx][7]\n",
    "    \n",
    "        return states, actions, rewards, next_states, dones, humans, next_humans, goals\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.current_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649e0fe-ad28-4d7d-8092-1b9188ce2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgentWithAttention:\n",
    "    def __init__(self, state_size, action_size, human_dim, num_humans, goal_dim, buffer_size, batch_size, lr_actor, lr_critic, gamma, tau):\n",
    "        self.actor = Actor(state_size, action_size, num_humans=num_humans)\n",
    "        self.actor_target = Actor(state_size, action_size, num_humans=num_humans)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(state_size, action_size, human_size=human_dim * num_humans, goal_size=goal_dim)\n",
    "        self.critic_target = Critic(state_size, action_size, human_size=human_dim * num_humans, goal_size=goal_dim)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = ReplayBuffer(max_size=buffer_size, batch_size=batch_size,\n",
    "                                          state_size=state_size, action_size=action_size,\n",
    "                                          human_size=human_dim * num_humans, goal_size=goal_dim)\n",
    "\n",
    "        # Initialize target networks with same weights as original networks\n",
    "        self._update_targets(self.actor_target, self.actor)\n",
    "        self._update_targets(self.critic_target, self.critic)\n",
    "    \n",
    "    def _update_targets(self, target_model, model):\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def act(self, state, humans, goal):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        humans = torch.FloatTensor(humans).unsqueeze(0)\n",
    "        goal = torch.FloatTensor(goal).unsqueeze(0)\n",
    "        \n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action, _, _ = self.actor(state, humans, goal)\n",
    "        self.actor.train()\n",
    "        \n",
    "        return action.squeeze(0).numpy()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done, humans, next_humans, goal):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done, humans, next_humans, goal)\n",
    "\n",
    "  \n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample from memory\n",
    "        states, actions, rewards, next_states, dones, humans, next_humans, goals = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "        humans = torch.tensor(humans, dtype=torch.float).to(device)\n",
    "        next_humans = torch.tensor(next_humans, dtype=torch.float).to(device)\n",
    "        goals = torch.tensor(goals, dtype=torch.float).to(device)\n",
    "    \n",
    "        # Get next actions\n",
    "        next_actions = self.actor_target(next_states, next_humans, goals)\n",
    "        if isinstance(next_actions, tuple):\n",
    "            next_actions = next_actions[0]\n",
    "        \n",
    "        # Compute Q targets\n",
    "        Q_targets_next = self.critic_target(next_states, next_actions, next_humans, goals)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute Q expected\n",
    "        Q_expected = self.critic(states, actions, humans, goals)\n",
    "        # Q_targets = Q_targets.view_as(Q_expected)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor(states, humans, goals)\n",
    "        if isinstance(actions_pred, tuple):\n",
    "            actions_pred = actions_pred[0]\n",
    "        \n",
    "        actor_loss = -self.critic(states, actions_pred, humans, goals).mean()\n",
    "        \n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update the target networks\n",
    "        self.soft_update(self.critic, self.critic_target)\n",
    "        self.soft_update(self.actor, self.actor_target)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau=1e-3):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "    # def _soft_update_target(self, target_model, model, tau):\n",
    "    #     for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "    #         target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12af99-38c2-41db-ab20-742c3b7595dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = ContinuousRobotNavigationEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    human_dim = 2  # Assuming each human is represented by (x, y) coordinates\n",
    "    num_humans = env.num_humans\n",
    "    goal_dim = 2  # Assuming goal is represented by (x, y) coordinates\n",
    "    \n",
    "    buffer_size = 10000\n",
    "    batch_size = 64\n",
    "    lr_actor = 1e-3\n",
    "    lr_critic = 1e-3\n",
    "    gamma = 0.99\n",
    "    tau = 1e-3\n",
    "\n",
    "    \n",
    "    ddpg_agent = DDPGAgentWithAttention(state_dim, action_dim, human_dim, num_humans, goal_dim,\n",
    "                                        buffer_size, batch_size, lr_actor, lr_critic, gamma, tau)\n",
    "    replay_buffer = ReplayBuffer(max_size=buffer_size, batch_size=batch_size,\n",
    "                                 state_size=state_dim, action_size=action_dim,\n",
    "                                 human_size=human_dim * num_humans, goal_size=goal_dim)\n",
    "    \n",
    "    # Training loop\n",
    "    num_episodes = 150\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        humans = np.array(env.human_positions).flatten()  # Flatten human positions\n",
    "        goal = np.array(env.goal_position)\n",
    "        episode_reward = 0\n",
    "        env.frames = []  # Clear frames for the new episode\n",
    "        \n",
    "        for t in range(100):\n",
    "            action = ddpg_agent.act(state, humans, goal)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_humans = np.array(env.human_positions).flatten()  # Update human positions\n",
    "            \n",
    "            ddpg_agent.remember(state, action, reward, next_state, done, humans, next_humans, goal)\n",
    "            \n",
    "            state = next_state\n",
    "            humans = next_humans\n",
    "            episode_reward += reward\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Learn from the experiences in the replay buffer\n",
    "        ddpg_agent.learn()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "        \n",
    "        # Save GIF\n",
    "        imageio.mimsave(f'episode_{episode}.gif', env.frames, fps=10)\n",
    "    \n",
    "    plt.plot(range(num_episodes), episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward over Episodes')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ab292-b131-4e9a-8892-a2e16a0de6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
