{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fdfec-890e-4c78-b935-2e67a4a3318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym  # Assuming you have a gym-like environment\n",
    "from gym import spaces\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import imageio.v2 as imageio\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import FancyArrow\n",
    "from shapely.geometry import Point, LineString, box\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.distributions import Gumbel\n",
    "from torch_geometric.nn import GATConv\n",
    "import math\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b81eb-6f70-4015-a8c4-2ea4ddd4e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GST and attention based model integration\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels, out_channels, heads=heads, concat=True, dropout=0.6)\n",
    "        self.fc_out = nn.Linear(out_channels * heads, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat_conv(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class InteractionBasedAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers):\n",
    "        super(InteractionBasedAttentionModel, self).__init__()\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            GAT(in_channels=input_dim if i == 0 else hidden_dim, out_channels=hidden_dim, heads=num_heads)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.gumbel_softmax = GumbelSoftmax()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x = gat_layer(x, edge_index)\n",
    "        x = self.fc_out(x)\n",
    "        return self.gumbel_softmax(x)\n",
    "\n",
    "    def predict(self, agent_positions, agent_velocities):\n",
    "        x = torch.tensor(agent_positions + agent_velocities, dtype=torch.float32).to(device)\n",
    "        edge_index = self.construct_edge_index(len(agent_positions) + len(agent_velocities))\n",
    "        predicted_trajectories = self.forward(x, edge_index)\n",
    "        return predicted_trajectories.detach().cpu().numpy()\n",
    "\n",
    "    def construct_edge_index(self, num_agents):\n",
    "        edge_index = []\n",
    "        for i in range(num_agents):\n",
    "            for j in range(num_agents):\n",
    "                if i != j:\n",
    "                    edge_index.append([i, j])\n",
    "        return torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "\n",
    "class GumbelSoftmax(nn.Module):\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super(GumbelSoftmax, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20):\n",
    "        U = torch.rand(shape).to(device)\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits):\n",
    "        y = logits + self.sample_gumbel(logits.size())\n",
    "        return F.softmax(y / self.temperature, dim=-1)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return self.gumbel_softmax_sample(logits)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class EnhancedGumbelSocialTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, output_dim, max_len=100):\n",
    "        super(EnhancedGumbelSocialTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.gumbel_softmax = GumbelSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc_out(x)\n",
    "        return self.gumbel_softmax(x)\n",
    "\n",
    "    def predict(self, agent_positions, agent_velocities):\n",
    "        input_data = torch.tensor(agent_positions + agent_velocities, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        predicted_trajectories = self.forward(input_data)\n",
    "        return predicted_trajectories.squeeze(0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6b0bb-c940-4c64-a007-f132a0afd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment\n",
    "class ContinuousRobotNavigationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def is_goal_behind_wall(self, goal_position):\n",
    "        # Check if goal_position is behind any wall\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if goal_position[0] >= x and goal_position[0] <= x + width \\\n",
    "                    and goal_position[1] >= y and goal_position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "    def __init__(self, interaction_model):\n",
    "        super(ContinuousRobotNavigationEnv, self).__init__()\n",
    "        self.grid_size = 10\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size, shape=(2,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)  # Continuous action space\n",
    "\n",
    "        self.velocity_scale = 0.3\n",
    "        self.lidar_range = 3.0\n",
    "\n",
    "        self.num_humans = 5  # Number of humans\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        \n",
    "        # self.num_obstacles = 3  # Number of static obstacles\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "\n",
    "        self.maze_walls = [\n",
    "            (5, 8, 0.3, 1.5),   # Example wall 1\n",
    "            (0, 7, 1.5, 0.3),   # Example wall 2\n",
    "            (6.5, 6.5, 1.5, 0.3),   # Example wall 3\n",
    "            (4, 4.5, 0.3, 1.5),   # Example wall 4\n",
    "            (1.5, 2, 0.3, 1.5), #5th wall\n",
    "            (5, 2.5, 1.5, 0.3),  #6th wall\n",
    "            (8, 4, 1.5, 0.3),    #7th wall\n",
    "            (7, 0, 0.3, 1.5)   #8th wall    \n",
    "        ]\n",
    "\n",
    "        \n",
    "        \n",
    "        self.goal_position = (0, 0)\n",
    "\n",
    "        self.frames = []  # Initialize the frames list\n",
    "        self.interaction_model = interaction_model\n",
    "\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def _random_position(self):\n",
    "        return [np.random.uniform(0, self.grid_size - 1), np.random.uniform(0, self.grid_size - 1)]\n",
    "\n",
    "    def _random_velocity(self):\n",
    "        # return [np.random.uniform(-1, 1), np.random.uniform(-1, 1)]\n",
    "        return [np.random.uniform(-1, 1) * self.velocity_scale, np.random.uniform(-1, 1) * self.velocity_scale]\n",
    "\n",
    "    def reset(self):\n",
    "        self.robot_position = [0, 0]\n",
    "        self.robot_orientation = np.pi/4  #looking diagonally up\n",
    "        self.human_positions = [self._random_position() for _ in range(self.num_humans)]\n",
    "        self.human_velocities = [self._random_velocity() for _ in range(self.num_humans)]\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]\n",
    "        # self.obstacle_positions = [self._random_position() for _ in range(self.num_obstacles)]\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # self.goal_position = self._random_position()\n",
    "        while True:\n",
    "            self.goal_position = self._random_position()\n",
    "            if not self.is_goal_behind_wall(self.goal_position):\n",
    "                break\n",
    "        self.frames = []  # Reset frames list at the start of each episode\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def check_wall_collision(self, position):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.1) and position[0] <= x + width and position[1] >= (y-0.1) and position[1] <= y + height:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def prepare_trajectory_data(self):\n",
    "        agent_positions = self.human_positions + [self.robot_position]\n",
    "        agent_velocities = self.human_velocities + [[0, 0]]  # Robot velocity can be assumed as [0, 0] initially\n",
    "        return agent_positions, agent_velocities\n",
    "        \n",
    "    def predict_human_positions(self):\n",
    "        self.predicted_human_positions = [[] for _ in range(self.num_humans)]  # Reset predicted positions\n",
    "        num_predictions = 5  # Number of future positions to predict\n",
    "        \n",
    "        for i in range(self.num_humans):\n",
    "            future_positions = []\n",
    "            future_position = np.array(self.human_positions[i])\n",
    "            for _ in range(num_predictions):\n",
    "                future_position += np.array(self.human_velocities[i])\n",
    "                future_position[0] = np.clip(future_position[0], 0, self.grid_size - 1)\n",
    "                future_position[1] = np.clip(future_position[1], 0, self.grid_size - 1)\n",
    "                future_positions.append(future_position.copy())\n",
    "        \n",
    "            # Update current position\n",
    "            self.human_positions[i][0] += self.human_velocities[i][0]\n",
    "            self.human_positions[i][1] += self.human_velocities[i][1]\n",
    "        \n",
    "            # Check for boundary collisions and reverse velocity if necessary\n",
    "            if self.human_positions[i][0] < 0 or self.human_positions[i][0] >= self.grid_size:\n",
    "                self.human_velocities[i][0] *= -1\n",
    "            if self.human_positions[i][1] < 0 or self.human_positions[i][1] >= self.grid_size:\n",
    "                self.human_velocities[i][1] *= -1\n",
    "\n",
    "            if self.check_wall_collision(self.human_positions[i]):\n",
    "                self.human_velocities[i][0] *= -1\n",
    "                self.human_velocities[i][1] *= -1\n",
    "        \n",
    "            # Update future positions if within lidar range\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(self.human_positions[i])) <= self.lidar_range:\n",
    "                self.predicted_human_positions[i] = future_positions\n",
    "        \n",
    "\n",
    "\n",
    "    def check_collision(self, position):\n",
    "        # Check collision with humans\n",
    "        for human_pos in self.human_positions:\n",
    "            if np.linalg.norm(np.array(self.robot_position) - np.array(human_pos)) < 1.0:  # Collision threshold\n",
    "                return True\n",
    "        \n",
    "        # Check collision with obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     if np.linalg.norm(np.array(self.robot_position) - np.array(obstacle_pos)) < 1.0:  # Collision threshold\n",
    "        #         return True\n",
    "\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if position[0] >= (x-0.3) and position[0] <= x + width and position[1] >= (y-0.3) and position[1] <= y + height:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # Apply scaling to the action to reduce velocity\n",
    "        scaled_action = np.array(action) * self.velocity_scale\n",
    "    \n",
    "        # Calculate the new robot position\n",
    "        new_robot_position = [\n",
    "            self.robot_position[0] + scaled_action[0],\n",
    "            self.robot_position[1] + scaled_action[1]\n",
    "        ]\n",
    "    \n",
    "        # Clip the new position to stay within the environment bounds\n",
    "        new_robot_position = [\n",
    "            np.clip(new_robot_position[0], 0, self.grid_size - 1),\n",
    "            np.clip(new_robot_position[1], 0, self.grid_size - 1)\n",
    "        ]\n",
    "    \n",
    "        # Check if the new position collides with maze walls\n",
    "        if not self.check_collision(new_robot_position):\n",
    "            self.robot_position = new_robot_position\n",
    "        else:\n",
    "            done = True\n",
    "            return self.state, -10, done, {}\n",
    "    \n",
    "        if np.linalg.norm(scaled_action) > 0:\n",
    "            self.robot_orientation = np.arctan2(scaled_action[1], scaled_action[0])\n",
    "\n",
    "        agent_positions, agent_velocities = self.prepare_trajectory_data()\n",
    "        predicted_trajectories = self.interaction_model.predict(agent_positions, agent_velocities)\n",
    "        # print(predicted_trajectories)\n",
    "        self.predict_human_positions()\n",
    "        \n",
    "        goal_threshold = 0.7\n",
    "        distance_to_goal = np.linalg.norm(np.array(self.robot_position) - np.array(self.goal_position))\n",
    "\n",
    "        reward = -distance_to_goal\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if distance_to_goal < 0.7: \n",
    "            reward += 100  \n",
    "            done = True\n",
    "        else:\n",
    "            collision = self.check_collision(self.robot_position)\n",
    "            if collision:\n",
    "                reward -= 10  \n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "    \n",
    "        # if done:\n",
    "        #     self.reset()  \n",
    "        \n",
    "        self.state = np.array(self.robot_position, dtype=np.float32)\n",
    "        # print(self.state)\n",
    "        info = {}  # Add any additional info you want to pass\n",
    "    \n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "        # Draw the goal position\n",
    "        goal = plt.Circle(self.goal_position, 0.5, color='green')\n",
    "        ax.add_artist(goal)\n",
    "    \n",
    "        # Draw the robot\n",
    "        robot = plt.Circle(self.robot_position, 0.5, color='red')\n",
    "        ax.add_artist(robot)\n",
    "    \n",
    "        # Draw the humans\n",
    "        for human_pos in self.human_positions:\n",
    "            human = plt.Circle(human_pos, 0.5, color='blue')\n",
    "            ax.add_artist(human)\n",
    "\n",
    "        \n",
    "        #static obstacles\n",
    "        # for obstacle_pos in self.obstacle_positions:\n",
    "        #     obstacle = plt.Circle(obstacle_pos, 0.5, color='black')\n",
    "        #     ax.add_artist(obstacle)\n",
    "\n",
    "        #walls\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            rect = plt.Rectangle((x, y), width, height, color='black')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "        \n",
    "        #Orientation\n",
    "        arrow_length = 0.7\n",
    "        dx = arrow_length * np.cos(self.robot_orientation)\n",
    "        dy = arrow_length * np.sin(self.robot_orientation)\n",
    "        arrow = FancyArrow(self.robot_position[0], self.robot_position[1], dx, dy, color='black', width=0.1)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "        #Lidar range\n",
    "        # lidar_circle = plt.Circle(self.robot_position, self.lidar_range, color='gray', alpha=0.2)\n",
    "        # ax.add_artist(lidar_circle)\n",
    "        self.render_lidar(ax)\n",
    "\n",
    "        #Trajectory prediction\n",
    "        for predicted_positions in self.predicted_human_positions:\n",
    "            for future_pos in predicted_positions:\n",
    "                future_circle = plt.Circle(future_pos, 0.2, color='pink', alpha=0.3)\n",
    "                ax.add_artist(future_circle)\n",
    "\n",
    "        \n",
    "    \n",
    "        plt.axis('off')\n",
    "    \n",
    "        # Save frame to list\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        frame = imageio.imread(buf)\n",
    "        self.frames.append(frame)\n",
    "        buf.close()\n",
    "\n",
    "        plt.close()  \n",
    "\n",
    "        # Show the plot in real-time\n",
    "        # plt.imshow(frame)\n",
    "        # plt.axis('off')\n",
    "        # plt.pause(0.1)\n",
    "        # plt.clf()\n",
    "\n",
    "    def render_lidar(self, ax):\n",
    "        lidar_segments = 36\n",
    "        angle_increment = 2 * np.pi / lidar_segments\n",
    "        for i in range(lidar_segments):\n",
    "            angle = i * angle_increment\n",
    "            end_point = self.robot_position + np.array([np.cos(angle), np.sin(angle)]) * self.lidar_range\n",
    "            ray_end_point = self.get_ray_end_point(self.robot_position, end_point)\n",
    "            ax.plot([self.robot_position[0], ray_end_point[0]], [self.robot_position[1], ray_end_point[1]], color='dimgray', alpha=0.2)\n",
    "\n",
    "    def get_ray_end_point(self, start, end):\n",
    "        for wall in self.maze_walls:\n",
    "            x, y, width, height = wall\n",
    "            if self.line_intersects_rect(start, end, x, y, width, height):\n",
    "                return self.get_intersection_point(start, end, x, y, width, height)\n",
    "        return end\n",
    "\n",
    "    def line_intersects_rect(self, start, end, x, y, width, height):\n",
    "        def ccw(A, B, C):\n",
    "            return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "\n",
    "        A = start\n",
    "        B = end\n",
    "        C1 = [x, y]\n",
    "        C2 = [x + width, y]\n",
    "        C3 = [x, y + height]\n",
    "        C4 = [x + width, y + height]\n",
    "        return (ccw(A, C1, C2) != ccw(B, C1, C2) and ccw(A, B, C1) != ccw(A, B, C2)) or \\\n",
    "               (ccw(A, C3, C4) != ccw(B, C3, C4) and ccw(A, B, C3) != ccw(A, B, C4)) or \\\n",
    "               (ccw(A, C1, C3) != ccw(B, C1, C3) and ccw(A, B, C1) != ccw(A, B, C3)) or \\\n",
    "               (ccw(A, C2, C4) != ccw(B, C2, C4) and ccw(A, B, C2) != ccw(A, B, C4))\n",
    "\n",
    "    def get_intersection_point(self, start, end, x, y, width, height):\n",
    "        def line(p1, p2):\n",
    "            A = (p1[1] - p2[1])\n",
    "            B = (p2[0] - p1[0])\n",
    "            C = (p1[0] * p2[1] - p2[0] * p1[1])\n",
    "            return A, B, -C\n",
    "\n",
    "        def intersection(L1, L2):\n",
    "            D = L1[0] * L2[1] - L1[1] * L2[0]\n",
    "            Dx = L1[2] * L2[1] - L1[1] * L2[2]\n",
    "            Dy = L1[0] * L2[2] - L1[2] * L2[0]\n",
    "            if D != 0:\n",
    "                return Dx / D, Dy / D\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        walls = [\n",
    "            ([x, y], [x + width, y]),\n",
    "            ([x, y], [x, y + height]),\n",
    "            ([x + width, y], [x + width, y + height]),\n",
    "            ([x, y + height], [x + width, y + height])\n",
    "        ]\n",
    "        closest_point = end\n",
    "        min_distance = np.linalg.norm(np.array(start) - np.array(end))\n",
    "        for wall in walls:\n",
    "            L1 = line(start, end)\n",
    "            L2 = line(wall[0], wall[1])\n",
    "            R = intersection(L1, L2)\n",
    "            if R:\n",
    "                dist = np.linalg.norm(np.array(start) - np.array(R))\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_point = R\n",
    "        return closest_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052806a-327a-4265-9989-df1bd4c1efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Output action is in range [-1, 1]\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48428a59-8706-43db-ae67-c36f1a63bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649e0fe-ad28-4d7d-8092-1b9188ce2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, buffer_size, batch_size, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, tau=1e-3, huber_delta=1.0, target_update_freq=10, noise_std=0.05):\n",
    "        self.actor = Actor(state_size, action_size)\n",
    "        self.actor_target = Actor(state_size, action_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(state_size, action_size)\n",
    "        self.critic_target = Critic(state_size, action_size)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, batch_size, state_size, action_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.huber_delta = huber_delta\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.step_counter = 0\n",
    "        self.noise = OUNoise(action_size, sigma=noise_std)\n",
    "        \n",
    "        # Initialize target networks with same weights as original networks\n",
    "        self._update_targets(self.actor_target, self.actor)\n",
    "        self._update_targets(self.critic_target, self.critic)\n",
    "\n",
    "        #losses\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "    \n",
    "    def _update_targets(self, target_model, model):\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = self.actor(state).detach().numpy().flatten()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()  # Add noise\n",
    "        return np.clip(action, -1, 1)  # Clip action to be within valid range\n",
    "        # return self.actor(state).detach().numpy()\n",
    "        \n",
    "    #Test\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    #Learn\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = np.array(states)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = np.array(actions)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = np.array(next_states)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        \n",
    "        # Critic update\n",
    "        Q_targets_next = self.critic_target(next_states, self.actor_target(next_states))\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic(states, actions)\n",
    "        # critic_loss = F.mse_loss(Q_expected, Q_targets.detach())\n",
    "        critic_loss = F.smooth_l1_loss(Q_expected, Q_targets.detach()) #huber loss\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)  # Gradient clipping\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor update\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)  # Gradient clipping\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        \n",
    "        #Update target networks based on step counter\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter % self.target_update_freq == 0:\n",
    "            self._soft_update_target(self.actor_target, self.actor, self.tau)\n",
    "            self._soft_update_target(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "\n",
    "    def plot_loss(self, save_path=None):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.actor_losses, label='Actor Loss')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Actor Loss')\n",
    "        plt.savefig('Actor_losses.png')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.critic_losses, label='Critic Loss')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Critic Loss')\n",
    "        plt.savefig('Critic_losses.png')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _soft_update_target(self, target_model, model, tau):\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e172b2-ae52-4933-903b-83a0495d42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, batch_size, state_size, action_size):\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer = []\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.idx] = experience\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in batch])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ad112-fa1d-4679-a06c-1ea38a18a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(actor, critic, actor_target, critic_target, actor_optimizer, critic_optimizer, filepath):\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_target_state_dict': actor_target.state_dict(),\n",
    "        'critic_target_state_dict': critic_target.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, actor, critic, actor_target, critic_target, actor_optimizer, critic_optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "    actor_target.load_state_dict(checkpoint['actor_target_state_dict'])\n",
    "    critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "    actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "    critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "    print(f\"Checkpoint loaded from {filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73b641-d317-4ec9-83a6-f77d54662a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses\n",
    "def save_loss_data(filename, actor_losses, critic_losses):\n",
    "    loss_data = {\n",
    "        \"actor_losses\": actor_losses,\n",
    "        \"critic_losses\": critic_losses\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(loss_data, f)\n",
    "\n",
    "def load_loss_data(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            loss_data = json.load(f)\n",
    "        return loss_data[\"actor_losses\"], loss_data[\"critic_losses\"]\n",
    "    else:\n",
    "        return [], []\n",
    "#Episode count\n",
    "def save_episode_count(filename, episode_count):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump({'episode_count': episode_count}, f)\n",
    "\n",
    "def load_episode_count(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('episode_count', 0)\n",
    "    return 0\n",
    "#Rewards\n",
    "def save_rewards_data(filepath, episode_rewards):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(episode_rewards, f)\n",
    "\n",
    "def load_rewards_data(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff1ae9-911e-487e-a747-f87d9a24ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints'\n",
    "rewards_data_filepath = os.path.join(checkpoint_dir, 'rewards_data.json')\n",
    "episode_rewards = load_rewards_data(rewards_data_filepath)\n",
    "print(len(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad12af99-38c2-41db-ab20-742c3b7595dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InteractionBasedAttentionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# ddpg_agent.plot_loss()\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     interaction_model \u001b[38;5;241m=\u001b[39m \u001b[43mInteractionBasedAttentionModel\u001b[49m(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      3\u001b[0m     interaction_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m ContinuousRobotNavigationEnv(interaction_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InteractionBasedAttentionModel' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    interaction_model = InteractionBasedAttentionModel(input_dim=2, hidden_dim=128, output_dim=2, num_heads=4, num_layers=6)\n",
    "    interaction_model.to(device)\n",
    "    env = ContinuousRobotNavigationEnv(interaction_model)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    buffer_size = 10000\n",
    "    batch_size = 64\n",
    "    lr_actor = 1e-3\n",
    "    lr_critic = 1e-3\n",
    "    gamma = 0.99\n",
    "    tau = 1e-3\n",
    "\n",
    "    ddpg_agent = DDPGAgent(state_dim, action_dim, buffer_size, batch_size, lr_actor, lr_critic, gamma, tau)\n",
    "    replay_buffer = ReplayBuffer(max_size=buffer_size, batch_size=batch_size, state_size=state_dim, action_size=action_dim)\n",
    "    # replay_buffer = PrioritizedReplayBuffer(capacity=100000)\n",
    "\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Define checkpoint filename with timestamp\n",
    "    checkpoint_filename = f\"ddpg_checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\"\n",
    "    checkpoint_filepath = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "\n",
    "    # if os.path.exists(checkpoint_filepath):\n",
    "    #     load_checkpoint(checkpoint_filepath, ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target,\n",
    "    #                     ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer)\n",
    "    \n",
    "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')], reverse=True)\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[0]) if checkpoint_files else None\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Loading checkpoint {latest_checkpoint}\")\n",
    "        load_checkpoint(latest_checkpoint, ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target,\n",
    "                        ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer)\n",
    "\n",
    "    loss_data_filepath = os.path.join(checkpoint_dir, 'loss_data.json')\n",
    "    ddpg_agent.actor_losses, ddpg_agent.critic_losses = load_loss_data(loss_data_filepath)\n",
    "\n",
    "    episode_count_filepath = os.path.join(checkpoint_dir, 'episode_count.json')\n",
    "    episode_count = load_episode_count(episode_count_filepath)\n",
    "\n",
    "    rewards_data_filepath = os.path.join(checkpoint_dir, 'rewards_data.json')\n",
    "    episode_rewards = load_rewards_data(rewards_data_filepath)\n",
    "\n",
    "    num_episodes_per_run = 8000\n",
    "    # total_episodes = 1000\n",
    "    # num_runs = (total_episodes - episode_count) // num_episodes_per_run\n",
    "\n",
    "    # print(f\"Starting training run with episode{num_runs+1}...\")\n",
    "\n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes_per_run):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        env.frames = []  # Clear frames for the new episode\n",
    "        for t in range(300):\n",
    "            action = ddpg_agent.act(state)\n",
    "            agent_positions, agent_velocities = env.prepare_trajectory_data()\n",
    "            predicted_trajectories = interaction_model.predict(agent_positions, agent_velocities)\n",
    "            # print(predicted_trajectories)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            ddpg_agent.remember(state, action, reward, next_state, done)  # Remember the experience\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            env.render()  \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if (episode+1) % 100 == 0:  # Save checkpoint every 100 episodes\n",
    "            save_checkpoint(ddpg_agent.actor, ddpg_agent.critic, ddpg_agent.actor_target, \n",
    "                            ddpg_agent.critic_target, ddpg_agent.actor_optimizer, ddpg_agent.critic_optimizer, checkpoint_filepath)\n",
    "            print(f'Saved checkpoint at episode {episode}')\n",
    "        \n",
    "        # Learn from the experiences in the replay buffer\n",
    "        ddpg_agent.learn()\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "        # Save GIF\n",
    "        imageio.mimsave(f'episode_{episode}.gif', env.frames, fps=10)\n",
    "\n",
    "        save_loss_data(loss_data_filepath, ddpg_agent.actor_losses, ddpg_agent.critic_losses)\n",
    "        save_rewards_data(rewards_data_filepath, episode_rewards)\n",
    "    episode_count += num_episodes_per_run\n",
    "    save_episode_count(episode_count_filepath, episode_count)\n",
    "        \n",
    "\n",
    "    plt.plot(range(len(episode_rewards)), episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward over Episodes')\n",
    "    plt.savefig('cumulative_reward_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    ddpg_agent.plot_loss(save_path=f'loss_plot_episode_{episode}.png')\n",
    "    # ddpg_agent.plot_loss()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47008c71-338b-4259-8052-f52f98f7f86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
